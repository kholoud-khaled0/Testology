import { Chapter } from "../types";

export const CHAPTER_3_PRACTICE: Chapter = {
  id: "genai-3",
  title: "Managing Risks of Generative AI in Software Testing",
  description: "Identifying, understanding, and mitigating risks associated with GenAI in software testing",
  icon: "ShieldCheck",
  questions: [

    // =========================
    // GenAI-3.1.1 (K1)
    // =========================
    {
      id: "q-1",
      code: "3.1.1.1",
      kLevel: "K1",
      text: "What is the definition of a hallucination in the context of Large Language Models?",
      options: [
        "The model's inability to process images in a multimodal context.",
        "Output generated by an LLM that appears factually incorrect or irrelevant to the task.",
        "A security breach where training data is leaked through a prompt.",
        "The probabilistic sampling process that causes different outputs for the same input."
      ],
      correctIndex: 1,
      explanation:
        "Hallucinations occur when an LLM generates output that appears factually incorrect or irrelevant to a given task."
    },
    {
      id: "q-2",
      code: "3.1.1.2",
      kLevel: "K1",
      text: "Which term describes a situation where an LLM misinterprets logical structures such as cause-and-effect or conditional logic?",
      options: [
        "Bias",
        "Hallucination",
        "Reasoning error",
        "Non-determinism"
      ],
      correctIndex: 2,
      explanation:
        "Reasoning errors occur when LLMs misinterpret logical structures such as cause-and-effect or conditional logic."
    },
    {
      id: "q-3",
      code: "3.1.1.3",
      kLevel: "K1",
      text: "In software testing, LLM biases typically originate from which of the following?",
      options: [
        "The use of a high temperature setting during inference.",
        "The data on which the model was trained.",
        "The size of the model's context window.",
        "The lack of a transformer architecture in the model."
      ],
      correctIndex: 1,
      explanation:
        "LLM biases originate from the data on which the model was trained."
    },

    // =========================
    // GenAI-3.1.2 (K3)
    // =========================
    {
      id: "q-4",
      code: "3.1.2.1",
      kLevel: "K3",
      text: "Scenario: You ask an LLM to generate test cases for a login screen. The LLM suggests a test case to \"Verify that the fingerprint scanner vibrates upon successful entry,\" even though the requirements document states the device has no biometric hardware. What has occurred?",
      options: [
        "A reasoning error in step-by-step problem solving.",
        "A bias favoring biometric security features.",
        "A hallucination creating fictitious test cases for non-existent criteria.",
        "A request manipulation attack."
      ],
      correctIndex: 2,
      explanation:
        "This is a hallucination where the LLM creates fictitious test cases for non-existent criteria."
    },
    {
      id: "q-5",
      code: "3.1.2.2",
      kLevel: "K3",
      text: "Scenario: You provide an LLM with a list of 50 test cases and ask it to prioritize them based on risk impact. The LLM places a \"Critical\" payment failure risk at the bottom of the list and a \"Low\" cosmetic UI change at the top, despite having correct risk data. Which issue is most likely present?",
      options: [
        "Data exfiltration",
        "A reasoning error in logical prioritization",
        "Data poisoning",
        "A hallucination about non-existent UI elements"
      ],
      correctIndex: 1,
      explanation:
        "This represents a reasoning error in logical prioritization."
    },
    {
      id: "q-6",
      code: "3.1.2.3",
      kLevel: "K3",
      text: "Scenario: When asked to generate diverse user personas for testing a global e-commerce app, the LLM only generates profiles for users from Western Europe and North America. This is an example of identifying:",
      options: [
        "A reasoning error in geographic calculations.",
        "A hallucination about global demographics.",
        "A bias favoring information dominant in its training data.",
        "A non-deterministic output variation."
      ],
      correctIndex: 2,
      explanation:
        "This is a bias originating from dominant patterns in the training data."
    },

    // =========================
    // GenAI-3.1.3 (K2)
    // =========================
    {
      id: "q-7",
      code: "3.1.3.1",
      kLevel: "K2",
      text: "How does prompt chaining serve as a mitigation technique for reasoning errors?",
      options: [
        "It allows the model to ignore its context window limits.",
        "It splits complex tasks into smaller steps, allowing for early detection of errors through verification.",
        "It ensures the model becomes 100% deterministic for all future prompts.",
        "It replaces the need for a reasoning-capable LLM with a foundation model."
      ],
      correctIndex: 1,
      explanation:
        "Prompt chaining splits complex tasks into smaller steps, allowing early error detection."
    },
    {
      id: "q-8",
      code: "3.1.3.2",
      kLevel: "K2",
      text: "Which of the following is a key technique to reduce the likelihood of hallucinations in GenAI results?",
      options: [
        "Increasing the temperature parameter to encourage more creative output.",
        "Providing complete context and relevant information within the prompt.",
        "Using only zero-shot prompting to avoid confusing the model.",
        "Relying exclusively on symbolic AI for all test tasks."
      ],
      correctIndex: 1,
      explanation:
        "Providing complete and relevant context reduces hallucinations."
    },
    {
      id: "q-9",
      code: "3.1.3.3",
      kLevel: "K2",
      text: "Why is comparing results across multiple LLMs recommended for software testing tasks?",
      options: [
        "It reduces the energy consumption of the individual models.",
        "It helps detect output errors and select the most reliable result.",
        "It eliminates the need for any human review or domain expertise.",
        "It increases the context window size by combining models."
      ],
      correctIndex: 1,
      explanation:
        "Comparing outputs helps detect errors and choose the most reliable result."
    },

    // =========================
    // GenAI-3.1.4 (K1)
    // =========================
    {
      id: "q-10",
      code: "3.1.4.1",
      kLevel: "K1",
      text: "Which parameter should be lowered to reduce randomness and achieve more consistent outputs from an LLM?",
      options: [
        "Context window",
        "Token count",
        "Temperature",
        "High-dimensional vectors"
      ],
      correctIndex: 2,
      explanation:
        "Lowering temperature reduces randomness and increases consistency."
    },
    {
      id: "q-11",
      code: "3.1.4.2",
      kLevel: "K1",
      text: "What is the purpose of setting random seeds in some LLM implementations?",
      options: [
        "To increase the diversity of generated test cases.",
        "To ensure the same pseudo-random sequence is used, improving reproducibility.",
        "To prevent the model from using its transformer architecture.",
        "To allow non-technical stakeholders to write code."
      ],
      correctIndex: 1,
      explanation:
        "Random seeds ensure reproducible outputs."
    },
    {
      id: "q-12",
      code: "3.1.4.3",
      kLevel: "K1",
      text: "Why is achieving consistent results with LLMs particularly challenging for long outputs?",
      options: [
        "Because long outputs require more symbolic logic rules.",
        "Because variability increases due to probabilistic sampling during inference.",
        "Because the model's energy consumption drops during long generations.",
        "Because the context window automatically resets every 100 tokens."
      ],
      correctIndex: 1,
      explanation:
        "Long outputs increase variability due to probabilistic sampling."
    },

    // =========================
    // GenAI-3.2.1 (K2)
    // =========================
    {
      id: "q-13",
      code: "3.2.1.1",
      kLevel: "K2",
      text: "In the context of testing with GenAI, what is a primary compliance risk?",
      options: [
        "Using a reasoning LLM instead of a foundation LLM.",
        "Using GenAI tools without complying with data protection regulations like GDPR.",
        "Generating too many test cases in a single sprint.",
        "Selecting a model with a context window smaller than 10,000 tokens."
      ],
      correctIndex: 1,
      explanation:
        "Non-compliance with data protection regulations poses legal risks."
    },
    {
      id: "q-14",
      code: "3.2.1.2",
      kLevel: "K2",
      text: "What does unintentional data exposure refer to in GenAI testing?",
      options: [
        "The model generating output that accidentally reveals sensitive or personally identifiable information.",
        "A developer forgetting to save a test script in the repository.",
        "A tester sharing a screenshot of a bug on a public social media platform.",
        "The model refusing to answer a prompt due to safety filters."
      ],
      correctIndex: 0,
      explanation:
        "Unintentional data exposure occurs when sensitive information is revealed in outputs."
    },
    {
      id: "q-15",
      code: "3.2.1.3",
      kLevel: "K2",
      text: "Why does the handling of sensitive information in LLM-powered test infrastructure require robust protection?",
      options: [
        "To prevent the model from experiencing reasoning errors.",
        "To prevent unauthorized access and exposure of confidential data.",
        "To ensure the model remains non-deterministic.",
        "To reduce the cost per 1,000 tokens."
      ],
      correctIndex: 1,
      explanation:
        "Robust protection prevents unauthorized access and data exposure."
    },

    // =========================
    // GenAI-3.2.2 (K2)
    // =========================
    {
      id: "q-16",
      code: "3.2.2.1",
      kLevel: "K2",
      text: "Which attack vector involves sending requests specifically designed to extract confidential training data from an LLM?",
      options: [
        "Request manipulation",
        "Data exfiltration",
        "Data poisoning",
        "Malicious code generation"
      ],
      correctIndex: 1,
      explanation:
        "Data exfiltration aims to extract confidential training data."
    },
    {
      id: "q-17",
      code: "3.2.2.2",
      kLevel: "K2",
      text: "What is an example of request manipulation in a GenAI testing process?",
      options: [
        "A tester manually editing a generated test script.",
        "Images that lure the AI into a different context to provoke hallucinations on acceptance criteria.",
        "Providing fake ratings to an AI-generated test report.",
        "Exceeding the context window to overload the memory."
      ],
      correctIndex: 1,
      explanation:
        "Request manipulation disrupts output by altering context."
    },
    {
      id: "q-18",
      code: "3.2.2.3",
      kLevel: "K2",
      text: "How is data poisoning exemplified in the sources?",
      options: [
        "Generating a backdoor communication channel to a malicious IP.",
        "Providing fake evaluations when rating the results of an AI-generated test report.",
        "Using an LLM to generate synthetic credit card numbers.",
        "Installing an LLM on a secure in-house server."
      ],
      correctIndex: 1,
      explanation:
        "Data poisoning involves manipulating training or feedback data."
    },

    // =========================
    // GenAI-3.2.3 (K2)
    // =========================
    {
      id: "q-19",
      code: "3.2.3.1",
      kLevel: "K2",
      text: "Which organizational strategy involves masking or replacing sensitive information with non-identifiable data?",
      options: [
        "Data minimization",
        "Data anonymization and pseudonymization",
        "Systematic review",
        "Model comparison"
      ],
      correctIndex: 1,
      explanation:
        "Anonymization and pseudonymization protect sensitive data."
    },
    {
      id: "q-20",
      code: "3.2.3.2",
      kLevel: "K2",
      text: "If a tester needs to use an LLM for highly confidential data, which operational environment is the most secure option?",
      options: [
        "A public AI chatbot with no privacy policy.",
        "Operating the LLM in a secure cloud or installing it in the organizationâ€™s own infrastructure.",
        "Sharing the data via a standard third-party API without encryption.",
        "Training a new foundation model using only public websites."
      ],
      correctIndex: 1,
      explanation:
        "Secure cloud or on-premise deployment offers higher protection."
    },
    {
      id: "q-21",
      code: "3.2.3.3",
      kLevel: "K2",
      text: "What is the role of systematic human review in mitigating GenAI risks in testing?",
      options: [
        "It is only necessary for zero-shot prompts.",
        "It is essential for ensuring the quality and accuracy of GenAI-powered test tasks.",
        "It replaces the need for security audits and vulnerability assessments.",
        "It is used to manually calculate the token count of every response."
      ],
      correctIndex: 1,
      explanation:
        "Human review ensures accuracy and quality."
    },

    // =========================
    // GenAI-3.3.1 (K2)
    // =========================
    {
      id: "q-22",
      code: "3.3.1.1",
      kLevel: "K2",
      text: "According to the sources, generating a single image using a powerful AI model can consume:",
      options: [
        "Negligible energy compared to text generation.",
        "As much energy as fully charging a smartphone.",
        "Less energy than sending a single email.",
        "Energy equivalent to running a data center for one hour."
      ],
      correctIndex: 1,
      explanation:
        "Image generation can consume energy equivalent to charging a smartphone."
    },
    {
      id: "q-23",
      code: "3.3.1.2",
      kLevel: "K2",
      text: "Which factor has a significant impact on the energy consumption of a GenAI task?",
      options: [
        "The number of testers in the organization.",
        "The complexity of the task and the computational resources required.",
        "The specific programming language of the test scripts.",
        "The color of the GUI wireframes being analyzed."
      ],
      correctIndex: 1,
      explanation:
        "Task complexity and compute requirements affect energy usage."
    },
    {
      id: "q-24",
      code: "3.3.1.3",
      kLevel: "K2",
      text: "What is a recommended best practice to mitigate the environmental impact of using GenAI in testing?",
      options: [
        "Generating thousands of redundant images for every test case.",
        "Limiting unnecessary model interactions.",
        "Using the largest possible LLM for simple data entry tasks.",
        "Running the LLM continuously without a context window."
      ],
      correctIndex: 1,
      explanation:
        "Limiting unnecessary interactions reduces energy consumption."
    },

    // =========================
    // GenAI-3.4.1 (K1)
    // =========================
    {
      id: "q-25",
      code: "3.4.1.1",
      kLevel: "K1",
      text: "Which standard specifies requirements for managing AI systems within an organization to promote consistency and reliability?",
      options: [
        "EU AI Act",
        "ISO/IEC 42001:2023",
        "NIST AI RMF",
        "ISO/IEC 23053:2022"
      ],
      correctIndex: 1,
      explanation:
        "ISO/IEC 42001:2023 specifies requirements for managing AI systems."
    },
    {
      id: "q-26",
      code: "3.4.1.2",
      kLevel: "K1",
      text: "What is the primary focus of the EU AI Act?",
      options: [
        "Providing guidelines for mathematical reasoning in LLMs.",
        "Establishing a legal framework that classifies AI applications by risk level.",
        "Defining the high-dimensional vector space for embeddings.",
        "Managing the tokenization process for multimodal models."
      ],
      correctIndex: 1,
      explanation:
        "The EU AI Act establishes a risk-based legal framework."
    },
    {
      id: "q-27",
      code: "3.4.1.3",
      kLevel: "K1",
      text: "Which framework focuses on managing AI risks with an emphasis on fairness, transparency, and security to prevent biased results?",
      options: [
        "NIST AI Risk Management Framework (US)",
        "ISO/IEC 23053:2022",
        "GDPR (Regulation EU 2016/679)",
        "The CT-GenAI Syllabus"
      ],
      correctIndex: 0,
      explanation:
        "NIST AI RMF focuses on fairness, transparency, and security."
    }

  ]
};
